---
title: "Red Wood Data Analysis Report"
author: "Andrew Amore, Dylan (Shi-Ting) Lu"
output:
  pdf_document: default
  html_document: default
classoption:
- twocolumn

header-includes: |
  \usepackage{titlesec}
  \titlespacing{\section}{0pt}{12pt plus 2pt minus 1pt}{0pt plus 1pt minus 1pt}
  \titlespacing{\subsection}{0pt}{12pt plus 2pt minus 1pt}{0pt plus 1pt minus 1pt}
  \titlespacing{\subsubsection}{0pt}{12pt plus 2pt minus 1pt}{0pt plus 1pt minus 1pt}
  
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
library(tidyverse)
library(ggplot2)
library(caret)
library(dplyr)
library(purrr)
library(lubridate) # for working with dates
library(ggplot2)  # for creating graphs
library(scales)   # to access breaks/formatting functions
library(gridExtra) # for arranging plots
```

## 1a Data Collection
### (a - Paper Summary)
The purpose of the study is twofold: capture information about a single redwood tree canopy over time and provide a roadmap for future macroscopic studies using a multi-sensor network.

The data was collected in Sonoma, California on a single Redwood tree over a period of days at consistent time intervals during the late spring/early summer.

Based on this study, researchers were able to verify the existence of dynamic spatio-temporal gradients surrounding the tree and prove that complex biological theories can be validated using this measurement framework. Researchers highlighted lesson's learned, beneficial for future studies, highlighting sensor sensitivity based on positioning and yield issues from memory/network constraints.

### (b - Data Collection)
### How are sensors deployed? 
Nodes (sensor housing) were attached to the body of one redwood tree at various radial, angular, and vertical distances. At  roughly 2-meter spacing intervals the first sensor was placed 15m from ground level and the last sensor at 70m above ground level. The majority of nodes used in the analysis were placed on the west side of the tree about 0.1m - 1m from the tree trunk. Several nodes were placed outside of the measurement envelope to monitor readings in the immediate vicinity.

Nodes had two means of capturing/transferring data: on-site logger stored readings on-device and a separate workflow transferred readings over-the-wire to gateway (make sure this is correct when looking at file).

Sensors were calibrated before being deployed in the field using two trials called roof and chamber. In the roof exercise, nodes were placed in direct view of sunlight atop a building to test the PAR measurements and which were compared to a well known reference. In the chamber phase, temperature and humidity sensors exposed to wide range of conditions: from 5-30 degrees Celsius and between 20-90 %RH. Before being deployed the data harvesting querying was tested in field on a sample tree in similar conditions to verify communication between nodes and on-site internet connected gateway.

### What is the duration of data recording?
Data was recorded over period of almost 44 days from 4/27/2004 5:10pm (epoch 1) to 6/10/2004 2:00pm (epoch 12635). Measurements were taken every 5 minutes and battery operated nodes were duty cycled to conserve power when not operating (on for 4 seconds to take measurements then turned off until next reading). 

### What are the main variables of interest?
Researchers were interested in traditional climate variables temperature, humidity, and light levels which were measured via Photosynthetically Active Radiation (PAR). Light wavelengths between 350nmâ€“700nm were captured in two measurements: incident (direct), which provides information about energy available for photosynthesis, and reflected (ambient), which was used for satellite validation of measurements.

### What is difference between data in two different files?
Data in "sonoma-data-log.csv" represents sensor data saved from the logger to the flash memory on the actual node that was retrieved after the deployment. Data in "sonoma-data-net-csv" is the data retrieved from the wireless network during the deployment.

The main difference between the files is that the scaling/precision on voltage measurements appear different between the two sources. The network file also appears to have some row duplication.

***

## 2 Data Cleaning
```{r, read-in-data, include=FALSE}
all <- read.csv("sonoma-data-all.csv")
log <- read.csv("sonoma-data-log.csv")
net <- read.csv("sonoma-data-net.csv")

# metadata files
dates <- read.table("sonoma-dates-formatted", sep=",", header = TRUE)   # losing some precision on epochDays

# get unique key for all data - prompt indicates composite key with two fields
primary_data_key <- unique(all[c("epoch", "nodeid")])
```

### (a - histograms of each variable in each file)
```{r, hist-of-variables}
# add data source column indicating file source for direct hist comparison
voltage_log_cutoff = max(log$voltage)
all = all %>%
  mutate(data_source = case_when(
    voltage > voltage_log_cutoff ~ "net",
    voltage <= voltage_log_cutoff ~ "log"
))

# epoch plot showing network failure
ggplot(all, aes(epoch, fill = data_source)) +
  geom_histogram(binwidth = 50) +
  ggtitle("Number of Entries by Epoch and Data Source ")
```
We see from above figure "Number of Entries by Epoch and Data Source" the range of epoch measurements and frequency are not consistent between the two data sources. Tolle cited wireless issues as potentially culprit as well as battery/memory capacity. 

```{r, show-net-key-duplicates}
# use primary composite key to plot duplicate readings by data source (same epoch and node id) there should only be one row per unique key in each data source
# plot 1 showing the breakdown by frequency (n is the number of duplicated rows)
all %>% 
  group_by(epoch, nodeid, data_source) %>%
  tally() %>%
  filter(n > 1) %>%
  ggplot(aes(n, fill = data_source)) +
    geom_histogram(binwidth = 1) +
    stat_bin(binwidth=1, geom="text", colour="black", size=3.5,
           aes(label=..count.., group=data_source), position=position_stack(vjust=0.5)) +
    ggtitle("Duplicate Entries by Data Source")

# plot 2 showing number of measurements that have duplicated rows
# summarize(sum(n)) adds the counts together and yields total duplicated rows
all %>% 
  group_by(epoch, nodeid, data_source) %>%
  tally() %>%
  filter(n > 1) %>%
  group_by(data_source) %>% 
  tally() %>%
  ggplot(aes(x=data_source, y=n, fill = data_source)) +
    geom_bar(stat = "identity") +
    geom_text(aes(label=n), position=position_dodge(width=0.9), vjust=-0.25) +
  ggtitle("Number of Measurements with Duplicated Readings")
# Coment
##  there might be some differences in the readings...will want to check that in addition to just the primary key duplication
```
We see in the above figure that there are duplicated measurement rows where the sensor recorded data multiple times for the same measurement interval. These duplicates should be removed before analysis.

```{r, voltage-hist-range-issue}
# voltage plot shows issue with units
ggplot(all, aes(voltage, fill = data_source)) +
  geom_histogram(binwidth = 50) +
  ggtitle("Voltage Units Differs Across Data Source")
```
Figure "Voltage Units Differs Across Data Sources" shows that the scale/units of voltage were not consistent between the two recording devices.


```{r, par-derivation}
# PAR Derivation Convert hamatop/hamabottom units
par_calibration_factor = 0.0185 # https://edstem.org/us/courses/7965/discussion/571902
all = all %>%
  mutate(incident_par = hamatop * par_calibration_factor,
         reflected_par = hamabot * par_calibration_factor
  )
```


```{r, all-hist-plots, include=FALSE}
# we need to add some text describing this if we want to keep it in report (i removed it). I think it's better to include in code file but not in write-up
all %>%
  keep(is.numeric) %>% 
  gather() %>% 
  ggplot(aes(value)) + 
    facet_wrap(~ key, scales = "free") + 
    geom_histogram()
```

### Which variable is not consistent?
The scaling and/or precision of voltage is not consistent between the two sources. Network file voltage is several orders of magnitude greater (readings in the hundreds) than the same readings in the log file (readings in the single digits). There are also large time gaps between the two files denoted by measurement count differences by epoch and data source as well as duplicated measurement rows.

### Convert the data to the same range. No code, but explain what we did?
We transformed the voltage readings in the network file by dividing it by 100. Then we combined the transformed network file and log file to get all.2 file with consistent variables.

```{r, covert-voltage-range}
net.2 = net %>%
  mutate(voltage = voltage / 100)
all.2 = rbind(net.2, log)

######### andrew adjustment - alter voltage in existing structure or create new objects?
# here i'm just altering in place
all = all %>%
  mutate(voltage = case_when(
    data_source == 'net' ~ voltage / 100
  ))

all %>%
  keep(is.numeric) %>% 
  gather() %>% 
  ggplot(aes(value)) + 
    facet_wrap(~ key, scales = "free") + 
    geom_histogram()

######### end update

all.2 %>%
  keep(is.numeric) %>% 
  gather() %>% 
  ggplot(aes(value)) + 
    facet_wrap(~ key, scales = "free") + 
    geom_histogram()
```

### (b - remove missing data)
We removed all readings that were taken when the node's voltage was higher than 3 volts or lower than 2.4 volts. 143312 readings were removed. 

```{r, remove-missing-data}
all.2 %>% 
  ggplot(aes(voltage)) + geom_histogram() + 
  ggtitle("Voltage Frequency Before Adjustment")

all.2 %>% 
  summary()
quantile(all.2$voltage, prob = c(.35,.97))

all.3 = all.2 %>%
  filter(voltage >= 2.4 & voltage <= 3)

all.3 %>% 
  ggplot(aes(voltage)) + geom_histogram() +
  ggtitle("Voltage Frequency After Adjustment")

nrow(all.2) - nrow(all.3)
```

```{r, remove-duplicate-readings}
# want to remove the duplicate readings talked about in the code block above

```


### Comment on the number of missing measurements and the corresponding date/time period?
There are 7568 row of readings containing at least a missing measurement. The missing measurements have corresponding date between May 25 to May 29 and at Nov 10.During May 25 to May 29, missing measurements occurred throughout the day. Less errors occurred at noon while more errors occurred at night and in the early morning. In Nov 10, most error occurred at 14:25:00. 

```{r}
all.4 = na.omit(all.3)
missings = all.3[rowSums(is.na(all.3)) > 0, ]
summary(missings)
missings.2 <- missings
missings.2$result_time <-as.Date(missings.2$result_time)
hist(missings.2$result_time, breaks = "months")

missings.3 = missings
missings.3$result_time <-ymd_hms(missings.3$result_time)
missings.3 %>% ggplot(aes(result_time, ..count..)) + 
    geom_histogram() +
    theme_bw() + xlab(NULL) +
    scale_x_datetime(breaks = date_breaks("3 months"), )

par(mfrow=c(3,3))
# 25
ggplot(missings.3, aes(x=result_time)) + 
  geom_histogram() +
  scale_x_datetime(limits = c(ymd_hms("2004-05-25 00:00:01"),ymd_hms("2004-05-25 23:59:59")))

# 26
ggplot(missings.3, aes(x=result_time)) + 
  geom_histogram() +
  scale_x_datetime(limits = c(ymd_hms("2004-05-26 00:00:01"),ymd_hms("2004-05-26 23:59:59")))
# 27
ggplot(missings.3, aes(x=result_time)) + 
  geom_histogram() +
  scale_x_datetime(limits = c(ymd_hms("2004-05-27 00:00:01"),ymd_hms("2004-05-27 23:59:59")))
# 28
ggplot(missings.3, aes(x=result_time)) + 
  geom_histogram() +
  scale_x_datetime(limits = c(ymd_hms("2004-05-28 00:00:01"),ymd_hms("2004-05-28 23:59:59")))
# 29
ggplot(missings.3, aes(x=result_time)) + 
  geom_histogram() +
  scale_x_datetime(limits = c(ymd_hms("2004-05-29 00:00:01"),ymd_hms("2004-05-29  23:59:59")))

#11-10
ggplot(missings.3, aes(x=result_time)) + 
  geom_histogram() +
  scale_x_datetime(limits = c(ymd_hms("2004-11-10 00:00:01"),ymd_hms("2004-11-10 23:59:59")))
```


### (c - incorporate location data from other file)
After we joined the location data, we have 15 variables.
```{r, incorporate-location-data}
locations <- read.table(file = "mote-location-data.txt", header = TRUE, sep = ",")
colnames(locations)[1] = "nodeid"
#Join data. Retain all rows in a that have a match in b.

## AA: could we use inner_join?
all.5 = left_join(all.4,locations, by = "nodeid")
all.6 = na.omit(all.5)
```


```{r, get-good-date-column, echo=FALSE}
# timestamps from the raw data files don't match the actually record epoch...use the dates file to update
all = all %>% 
  inner_join(dates, by = c("epoch" = "epochNums")) %>%
  select(-result_time) %>%
  relocate(epochDates, epochDays)

```


### (d - visually identify outliers for each of following: humidity, humid temp, hamatop, hamabot. Remove and comment on rationale for removing)
We can remove all humidity reading < 21 which is less than the 1% quantile, humid_temp > 100 which is larger than the 99% quantile, maybe hamabot > 4000, which is larger than the 98% quantile.

```{r, identify-outliers}
vars_want = all.6[c("humidity","humid_temp","hamatop","hamabot")]

vars_want %>%
  keep(is.numeric) %>% 
  gather() %>% 
  ggplot(aes(value)) + 
    facet_wrap(~ key, scales = "free") + 
    geom_histogram()

```

```{r,identify-outliers-quantiles}
quantile(vars_want$humidity, prob = c(.01,.95))
quantile(vars_want$humid_temp, prob = c(.05, .95))
quantile(vars_want$hamatop, prob = c(.05, .93))
quantile(vars_want$hamabot, prob = c(.05, .99))
```


```{r, identify-outliers-box-whisker}

boxplot(vars_want$humid_temp)
boxplot(vars_want$hamatop)
boxplot(vars_want$hamabot)

```



### (e bonus - discuss other possible outliers and explain reason why it is better to remove them than to keep)


***

## 3 Data Exploration

### (a) (Dylan)

### (b) (Dylan)

### (c) (Dylan)

### (d) (Dylan)

***

### (a) (Andrew)

### (b) (Andrew)

### (c) (Andrew)

### (d) (Andrew)

*** 

## 4 Interesting Findings


***

## 5 Graph Critique in the Paper