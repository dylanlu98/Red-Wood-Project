---
title: "Red Wood Data Analysis Report"
author: "Andrew Amore, Dylan (Shi-Ting) Lu"
output:
  pdf_document:
    fig_caption: true
classoption:
- twocolumn

header-includes: |
  \usepackage{titlesec}
  \titlespacing{\section}{0pt}{12pt plus 2pt minus 1pt}{0pt plus 1pt minus 1pt}
  \titlespacing{\subsection}{0pt}{12pt plus 2pt minus 1pt}{0pt plus 1pt minus 1pt}
  \titlespacing{\subsubsection}{0pt}{12pt plus 2pt minus 1pt}{0pt plus 1pt minus 1pt}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
library(tidyverse)
library(ggplot2)
library(caret)
library(reshape2)
library(GGally)
library(dplyr)
library(purrr)
library(lubridate) # for working with dates
library(ggplot2)  # for creating graphs
library(scales)   # to access breaks/formatting functions
library(gridExtra) # for arranging plots
```

## 1a Data Collection
### Paper Summary
The purpose of the study is twofold: capture information about a single redwood tree canopy over time and provide a roadmap for future macroscopic studies using a multi-sensor network.

The data was collected in Sonoma, California on a single Redwood tree over a period of days at consistent time intervals during the late spring/early summer.

Based on this study, researchers were able to verify the existence of dynamic spatio-temporal gradients surrounding the tree and prove that complex biological theories can be validated using this measurement framework. Researchers highlighted lesson's learned, beneficial for future studies, highlighting sensor sensitivity based on positioning and yield issues from memory/network constraints.

### Data Collection
### How are sensors deployed? 
Nodes (sensor housing) were attached to the body of one redwood tree at various radial, angular, and vertical distances. At  roughly 2-meter spacing intervals the first sensor was placed 15m from ground level and the last sensor at 70m above ground level. The majority of nodes used in the analysis were placed on the west side of the tree about 0.1m - 1m from the tree trunk. Several nodes were placed outside of the measurement envelope to monitor readings in the immediate vicinity.

Nodes had two means of capturing/transferring data: on-site logger stored readings on-device and a separate workflow transferred readings over-the-wire to gateway (make sure this is correct when looking at file).

Sensors were calibrated before being deployed in the field using two trials called roof and chamber. In the roof exercise, nodes were placed in direct view of sunlight atop a building to test the PAR measurements and which were compared to a well known reference. In the chamber phase, temperature and humidity sensors exposed to wide range of conditions: from 5-30 degrees Celsius and between 20-90 %RH. Before being deployed the data harvesting querying was tested in field on a sample tree in similar conditions to verify communication between nodes and on-site internet connected gateway.

### What is the duration of data recording?
Data was recorded over period of almost 44 days from 4/27/2004 5:10pm (epoch 1) to 6/10/2004 2:00pm (epoch 12635). Measurements were taken every 5 minutes and battery operated nodes were duty cycled to conserve power when not operating (on for 4 seconds to take measurements then turned off until next reading). 

### What are the main variables of interest?
Researchers were interested in traditional climate variables temperature, humidity, and light levels which were measured via Photosynthetically Active Radiation (PAR). Light wavelengths between 350nmâ€“700nm were captured in two measurements: incident (direct), which provides information about energy available for photosynthesis, and reflected (ambient), which was used for satellite validation of measurements.

### What is difference between data in two different files?
Data in "sonoma-data-log.csv" represents sensor data saved from the logger to the flash memory on the actual node that was retrieved after the deployment. Data in "sonoma-data-net-csv" is the data retrieved from the wireless network during the deployment.

The main difference between the files is that the scaling/precision on voltage measurements appear different between the two sources. The network file also appears to have some row duplication.

***

## 2 Data Cleaning
```{r, read-in-data, include=FALSE}
all <- read.csv("sonoma-data-all.csv")
log <- read.csv("sonoma-data-log.csv")
net <- read.csv("sonoma-data-net.csv")

# metadata files
dates <- read.table("sonoma-dates-formatted", sep=",", header = TRUE)   # losing some precision on epochDays
```


### (a - histograms of each variable in each file)
```{r, hist-of-variables}
# add data source column indicating file source for direct hist comparison
voltage_log_cutoff = max(log$voltage)
all = all %>%
  mutate(data_source = case_when(
    voltage > voltage_log_cutoff ~ "net",
    voltage <= voltage_log_cutoff ~ "log"
))
```

```{r, network-failure, fig.cap = "Number of Entries by Epoch and Data Source"}
# plot themes need work
# epoch plot showing network failure
ggplot(all, aes(x = epoch, color = data_source)) +
  geom_histogram(binwidth = 150) +
  # cosmetics
  labs(x="Epoch", y="Frequency", color="Data Source") +
  scale_color_brewer(palette="Spectral", labels=c("Logger", "Network"))
```

In Figure 1, the number of measurements recorded during each epoch is plotted for each data source: logger and network. Epoch range and record frequency are not consistent between the two, however, the distribution looks similar over the ranges where both have recorded data. The first network measurement occurs on epoch 2812 which corresponds to May 7th, almost 10 days after the experiment started. Tolle et al. cited packet drops and network related issues as potential culprit's to low network yield, but the data provided does not match the findings from Figure 7 in the original report. 

```{r, voltage-hist-range-issue, fig.cap = "Voltage Histogram - Units Differ by Data Source"}
# voltage plot shows issue with units
ggplot(all, aes(voltage, color = data_source, fill = data_source)) +
  geom_histogram(binwidth = 50) +
  
  labs(x="Voltage", y="Frequency", color="Data Source") +
  scale_color_brewer(palette="Spectral", labels=c("Logger", "Network"))+
  scale_fill_brewer(palette="Spectral", labels=c("Logger", "Network"))
```
Voltage units are not consistent across the two sources. Figure 2 plots the comparison and illustrates the scaling difference. The network voltage is several orders of magnitude greater than readings in the logger source. Voltage readings from the network source were divided by 100 to match the logger voltage units.

Hamatop and hamabot dimensions were converted to match the PAR units in the original paper, but values were consistent across sources.

$$ \text{Incident PAR} = hamatop*0.0185 $$
$$ \text{Reflected PAR} = hamabot *0.0185 $$

Each record, unique to each data source, has a composite key identifier: nodeid and epoch. Figure 3 shows the number of nodeid/epoch combinations that appear more than once in the dataset. 

```{r, show-net-key-duplicates, fig.cap = "Number of (Epoch, Nodeid) Duplicated Entries by Data Source"}
# plot 2 showing number of measurements that have duplicated rows
# summarize(sum(n)) adds the counts together and yields total duplicated rows
all %>% 
  group_by(epoch, nodeid, data_source) %>%
  tally() %>%
  filter(n > 1) %>%
  group_by(data_source) %>% 
  tally() %>%   # summarize(sum(n)) (sums )
  ggplot(aes(x=data_source, y=n, fill = data_source)) +
    geom_bar(stat = "identity") + 
    geom_text(aes(label=n), position=position_dodge(width=0.9), vjust=-0.25) +
    # cosmetics
    labs(x="Data Source", y="Record Count", color="Data Source") +
    theme(legend.position = "none") +
    scale_color_brewer(palette="Spectral")
```

Out of this duplicate subset how many are unique entries? A unique entry indicates some dimension doesn't match between the duplicate rows and could indicate a faulty sensor or measurement reading. Measures were averaged for the nodeid/epoch entries identified as distinct duplicates. 
![Some Examples of Distinct Duplicate Rows. Note Value Differences](./images/dupe_row.JPG)

```{r, dupe-rows-by-nodeid}
# data frame of the duplicates 
duplicate_entries = all %>% 
                      group_by(epoch, nodeid, data_source) %>%
                      tally() %>%
                      filter(n > 1) %>% select(epoch, nodeid, data_source) %>%
                      inner_join(all)

# how many are unique entries?
non_dist =  nrow(duplicate_entries) - nrow(distinct(duplicate_entries))
dist = nrow(distinct(duplicate_entries))
count_difference = data.frame(group = c("Non-Distinct", "Distinct"), value = c(non_dist, dist))

# plot in pie chart (maybe we could somehow include the overall sample size here)
ggplot(count_difference, aes(x="", y=value, fill=group)) +
  geom_bar(width = 1, stat = "identity") + 
  geom_text(aes(label = value),
            position = position_stack(vjust = 0.5)) +
  coord_polar("y", start=0)  + scale_fill_brewer(palette="Blues")+
  theme_minimal()
```
```{r, remove-duplicate-readings}
# take an average across all measures for subset that varies across composite keys.
# (voltage, depth, humidity, humid_temp, humid_adj, hamatop, hamabot)
dupes_dis = distinct(duplicate_entries)   # 22,587 composite key

# store the new values in dupe_dis and append it to all
dupe_dis = dupes_dis %>% 
  group_by(epoch, nodeid, data_source) %>% 
  summarize(mean_voltage = mean(voltage), 
            mean_depth = mean(depth),
            mean_humidity = mean(humidity),
            mean_humid_temp = mean(humid_temp),
            mean_humid_adj = mean(humid_adj),
            mean_hamatop = mean(hamatop),
            mean_hamabot = mean(hamabot),
            .groups = 'drop'
            ) %>%
  inner_join(dupes_dis) %>%
  select(epoch, nodeid, mean_voltage, mean_depth, mean_humidity, mean_humid_temp, mean_humid_adj, mean_hamatop, mean_hamabot, data_source) %>%
  rename(voltage = mean_voltage,
         depth = mean_depth,
         humidity = mean_humidity,
         humid_temp = mean_humid_temp,
         humid_adj = mean_humid_adj,
         hamatop = mean_hamatop,
         hamabot = mean_hamabot) %>% 
  distinct() %>%
  add_column(result_time = "", .before = "epoch") %>%
  add_column(parent = NA, .before = "voltage")
  

# adjust all...only include single entries
all = all %>% 
    group_by(epoch, nodeid, data_source) %>%
    tally() %>%
    filter(n == 1) %>% select(epoch, nodeid, data_source) %>%
    inner_join(all)

# union the new duplicate values
all = union(all, dupe_dis)
```


```{r, unit-conversion}
# PAR unit conversion - hamatop/hamabottom units
par_calibration_factor = 0.0185 # https://edstem.org/us/courses/7965/discussion/571902
all = all %>%
  mutate(incident_par = hamatop * par_calibration_factor,
         reflected_par = hamabot * par_calibration_factor
  )

# set all NAs to 0 
all$incident_par[is.na(all$incident_par)] = 0
all$reflected_par[is.na(all$reflected_par)] = 0

# voltage unit conversion
all = all %>%
  mutate(voltage = case_when(
    data_source == 'net' ~ voltage / 100,
    TRUE ~ voltage
  ))
```


```{r, get-good-date-column, echo=FALSE}
# update result time with accurate values from the metadata file.
all = all %>% 
  inner_join(dates, by = c("epoch" = "epochNums")) %>%
  select(-result_time) %>%
  relocate(epochDates, epochDays)

# split up the new date field by space...want to keep the time
all = all %>%
  separate(epochDates, into = c('weekday', 'month', 'day', 'time', 'year'),
           sep = " +")
```

```{r, remove-voltage-errors}
voltage_removed = all %>%
  filter(voltage > 3 | voltage < 2.4)

all = all %>%
  filter(voltage >= 2.4 & voltage <= 3)
```

### (b - remove missing data)
There are a large number of entries with completely missing measurement data.
```{r, remove-na-values}
# rows where one measure is na means all rows are na
#all %>% filter(is.na(humid_temp) & is.na(humidity) & is.na(hamatop) & is.na(hamabot))
all = all %>% drop_na(humid_temp) %>% drop_na(humidity) %>% drop_na(hamatop) %>% drop_na(hamabot)
```


Duplicates and entries with readings greater than 3V or lower than 2.4V were removed following procedure outlined in Tolle paper. Table XX lists the record count after each removal step.

**Data Removal Step**    **Record Count**   **Net Change**
---------------------   -----------------   --------------
Ingestion               416,036             0
Duplicate Removal       393,213             -22,823
Voltage Removal         264,144             -129,096
Drop NAs                256,628             -7,516
------------------      -----------------   --------------

### Comment on the number of missing measurements and the corresponding date/time period?
The time of day for the missing measurements (voltage only) appears uniform across the 24 hour range the sensors were active. Over the course of the experiment we see more missing measurements as the experiment goes on, which would correspond with the battery issue mentioned in the paper. We also see a fair amount of logger failures at the beginning of the experiment.


```{r, distibution-of-missing-measurements}
# time of day
ggplot(voltage_removed, aes(time)) +
    geom_bar(aes(fill = data_source)) +
    ggtitle("Time of Day - Missing Measurements")

# day
voltage_removed$date_concat = 
  as.Date(paste(voltage_removed$year, voltage_removed$month, voltage_removed$day), "%Y %b %d")

ggplot(voltage_removed, aes(date_concat)) +
    geom_bar(aes(fill = data_source)) +
    ggtitle("Day - Missing Measurements") +
    scale_x_date(breaks = scales::breaks_pretty(13))
```



### (c - incorporate location data from other file)
After we joined the location data, we have 15 variables.
```{r, incorporate-location-data}
locations <- read.table(file = "mote-location-data.txt", header = TRUE, sep = ",")
colnames(locations)[1] = "nodeid"

# this is dropping some records...not sure why
all = left_join(all, locations, by="nodeid")
```



### (d - visually identify outliers for each of following: humidity, humid temp, hamatop, hamabot. Remove and comment on rationale for removing)
Tolle mentioned two outlier related issues corresponding to humidity measurements: %RH greater than 100% and one node failing to track other measured values. 

We observed similar problems in our dataset. Visually inspecting the histogram we see a number of readings over 100%, clearly indicating a problem. Measurements corresponding to these readings were excluded from analysis.

Plotting individual nodes over time we identified nodeid 123 as the problem node (not tracking other readings) which starts to occur around epoch 5000. All sensor readings after this epoch were removed.

Visually inspecting temperature, we see a large number of outliers in the boxplot. A quick google search for hottest recorded temperature in Sonoma, CA is 44 degrees Celsius. All measurements greater than this cutoff were removed which deviates slightly from max value Tolle includes in final paper.

```{r, identify-outliers}
# raw histograms
all %>%
 ungroup() %>%
 select(humidity, humid_temp, incident_par, reflected_par, .group = ) %>%
 keep(is.numeric) %>%
 gather() %>%
 ggplot(aes(value)) +
   facet_wrap(~ key, scales = "free") +
   geom_histogram()

# remove readings with humidity > 100%
# this will remove all sensor readings...might only want to set to NA so other dimensions remain intact
all = all %>% filter(humidity <= 100)


# paper mentions one node not tracking humidity with others i think nodeid 123
all %>% filter(nodeid == 119 | nodeid == 123) %>%
ggplot(aes(x = epoch, y = humidity, color = nodeid)) +
  geom_line() +
  scale_color_gradientn(colours=rainbow(2)) +
  ggtitle("Node 119 and 123 Humidity Plots over Epoch")

# remove node 123 and epoch > 5000
all = all %>% filter((nodeid == 123 & epoch <= 5000) | (nodeid != 123))

# boxplots - use quantile function to remove
boxplot(all$humid_temp, main = "humid temp")
boxplot(all$humidity, main = "humidity")
boxplot(all$incident_par, main = "Incident PAR")
boxplot(all$reflected_par, main = "Reflected PAR")

# remove measurements based on temperature cutoff
# temp < 44
all = all %>% filter(humid_temp < 44)

boxplot(all$humid_temp, main = "humid temp - outliers removed")

## interquartile range - prompt says to visually inspect
# IQ_humid_temp = quantile(all$humid_temp, prob= c(0.01, 0.25, 0.75, 0.99))
# IQR = IQ_humid_temp["75%"] - IQ_humid_temp["25%"]
# 
# IQ_humidity = quantile(all$humidity, prob= c(0.01, 0.25, 0.75, 0.99))
# IQ_incident_par = quantile(all$incident_par, prob= c(0.01, 0.25, 0.75, 0.99))
# IQ_reflected_par = quantile(all$reflected_par, prob= c(0.01, 0.25, 0.75, 0.99))
```



### (e bonus - discuss other possible outliers and explain reason why it is better to remove them than to keep)
Duplication issue. NaN issue (all rows are blank).

***

## 3 Data Exploration
### (a)
We decided to look at two distinct time periods, sunrise and sunset, as we belived they presented the most dynamic conditions to explore potential correlations. Would correlations observed during one event also manifest during the other? Researching sunrise and sunset times in Sonoma, CA lead us to select 5-hour intervals encompassing both astrological events.

source: https://sunrise-sunset.org/us/sonoma-ca/2004/6

**Astrological Event**    **Start Time**      **End Time**
---------------------     --------------      --------------
Sunrise                   5:00am              10:00am           
Sunset                    4:00pm              9:00pm
---------------------     --------------      --------------

```{r, pairwise-scatterplots}
# "humidity","humid_temp","incident_par","reflected_par"

## only want one measurement per distinct time period, irrespective of the source...
## perform something similar to the dupe check above

# grab distinct time intervals
sunrise = all %>% filter(time >= "05:00" & time <= "10")
sunset = all %>% filter(time >= "16" & time <= "21")

ggcorr(all[c("humidity","humid_temp","incident_par", "reflected_par")], method = c("everything", "pearson")) +
  labs(title = "Correlation Map - All Time Periods")
ggcorr(sunrise[c("humidity","humid_temp","incident_par", "reflected_par")], method = c("everything", "pearson")) +
  labs(title = "Correlation Map - Sunrise")
ggcorr(sunset[c("humidity","humid_temp","incident_par", "reflected_par")], method = c("everything", "pearson")) +
  labs(title = "Correlation Map - Sunset")

########### Sunrise charts - 5:00am to 10:00am
## value dimension only
ggplot(sunrise, aes(x=humid_temp, y=humidity) ) +
  geom_point() +
  ggtitle("Humidity vs. Temperature - Sunrise")

ggplot(sunrise, aes(x=incident_par, y=humidity) ) +
  geom_point() +
  ggtitle("Humidity vs. Incident PAR - Sunrise")

ggplot(sunrise, aes(x=incident_par, y=humid_temp) ) +
  geom_point() +
  ggtitle("Temperature vs. Incident PAR - Sunrise")

ggplot(sunrise, aes(x=incident_par, y=reflected_par) ) +
  geom_point() +
  ggtitle("Reflected PAR vs. Incident PAR - Sunrise")

## height
ggplot(sunrise, aes(x=incident_par, y=Height) ) +
  geom_point() +
  ggtitle("Height vs. Incident PAR - Sunrise")


########### Sunset charts - 4:00pm to 9:00pm (16:00 - 21:00)
ggplot(sunset, aes(x=humid_temp, y=humidity) ) +
  geom_point() +
  ggtitle("Humidity vs. Temperature - Sunset")

ggplot(sunset, aes(x=incident_par, y=humidity) ) +
  geom_point() +
  ggtitle("Humidity vs. Incident PAR - Sunset")

ggplot(sunset, aes(x=incident_par, y=humid_temp) ) +
  geom_point() +
  ggtitle("Temperature vs. Incident PAR - Sunset")

ggplot(sunset, aes(x=incident_par, y=reflected_par) ) +
  geom_point() +
  ggtitle("Reflected PAR vs. Incident PAR - Sunset")

## height
ggplot(sunset, aes(x=incident_par, y=Height) ) +
  geom_point() +
  ggtitle("Height vs. Incident PAR - Sunset")

rm(sunrise)
rm(sunset)
```

Describe findings from the charts.


### (b) Any predictors associated with Incident PAR. If so explain the relationship.
Temperature appears to be good predictor of incident PAR. We saw positive correlations between the two variables during both time frames.

Humidity displayed a negative correlation with incident PAR which is more prenounced during the sunset window.

### (c) Value as Time Series

```{r, time-series-plots}
# value vs. time with height as color cue
# day plots are required bare minimum
# (Temperature, Relative Humidity, Incident PAR and Reflected PAR)

# convert day
all$date_concat =
  as.Date(paste(all$year, all$month, all$day), "%Y %b %d")

# try and bin heights (looking at distinct)
all = all %>% 
  mutate(height_class = case_when(
   Height <= 20 ~ "10 - 20 m",
   Height > 20 & Height <= 30 ~ "20 - 30 m",
   Height > 30 & Height <= 40 ~ "30 - 40 m",
   Height > 40 & Height <= 50 ~ "40 - 50 m",
   Height > 50 & Height <= 60 ~ "50 - 60 m",
   Height > 60  ~ ">60 m"))


# Temperature
ggplot(all, aes(x = date_concat, y = humid_temp, colour = height_class)) +
    geom_line() +
    ggtitle("Temperature vs. Time") +
    scale_x_date(breaks = scales::breaks_pretty(13))


# Humidity
ggplot(all, aes(x = date_concat, y = humidity, colour = height_class)) +
    geom_line() +
    ggtitle("Humidity vs. Time") +
    scale_x_date(breaks = scales::breaks_pretty(13))

# Incident PAR
ggplot(all, aes(x = date_concat, y = incident_par, colour = height_class)) +
    geom_line() +
    ggtitle("Incident PAR vs. Time") +
    scale_x_date(breaks = scales::breaks_pretty(13))

# Reflected PAR
ggplot(all, aes(x = date_concat, y = reflected_par, colour = height_class)) +
    geom_line() +
    ggtitle("Reflected PAR vs. Time") +
    scale_x_date(breaks = scales::breaks_pretty(13))


```



### (d) PCA Analysis & Scree Plot

```{r, pca-and-screeplot}


```


*** 

## 4 Interesting Findings
Idea List (things to investigate):
-   is a tree sentient? Does the weather from the previous n days affect any of the measurement readings collected? For example, would a series of dry days show any humidity differences in or around the tree?
-   Influence points. How much can the distributions be shifted by missing values (mentioned in article as future work). Does the missing data affect any of the findings?
-   How does the collected data match up with any weather trends in the area? Maybe the tree had higher humidity rates for example than normal ground.

***

## 5 Graph Critique in the Paper
### Log Transform of Histograms
The incident and reflected PAR histograms from Tolle et al. (figure 3a) have long tails and with large numbers of observations around 0 which make the charts hard to discern. We present this data after taking a log transform of the observations and present the results side-by-side with the initial chart versions (non-log scale). 
```{r, 5a-incident-charts}
# incident PAR charts

# need to account for the 0s...add small positive number

# Original Chart - No Log Transform
ggplot(all, aes(x = incident_par)) +
  geom_histogram(fill = "blue4", color = "white", 
                 aes(y = (..count..)/sum(..count..)), binwidth = 250) +
  scale_y_continuous(labels = scales::percent_format(), n.breaks = 10) +
  labs(title = 'Incident PAR - Original Scale') +
  xlab(expression('micromol/m'^2*'/s')) + 
  ylab("Percentage of Readings")

# Log Transform
ggplot(all, aes(x = log(incident_par + 0.001))) +
  geom_histogram(aes(y = (..count..)/sum(..count..)), binwidth = .5) +
  scale_y_continuous(labels = scales::percent_format()) +
  labs(title = 'Incident PAR - Log Scale') +
  xlab(expression('ln(micromol/m'^2*'/s)')) + 
  ylab("Percentage of Readings")

```
Both log charts visually convey the long tail better than the non-log counter-parts. In fact, the log scale displays a bimodal distributions that could be interpreted as light levels during the day and at night.

however, some readers might be mislead/confused by the log interpretation. At first glance, PAR measurements appear more concentrated around a smaller range and it's important readers understand the log function before reaching an incorrect conclusion derived from the plots. We've displayed both versions for clarity.

```{r, 5a-reflected-charts}
# reflected PAR
# Original Chart - No Log Transform
ggplot(all, aes(x = reflected_par)) +
  geom_histogram(fill = "blue4", color = "white", 
                 aes(y = (..count..)/sum(..count..)), binwidth = 25) +
  scale_y_continuous(labels = scales::percent_format(), n.breaks = 10) +
  labs(title = 'Reflected PAR - Original Scale') +
  xlab(expression('micromol/m'^2*'/s')) + 
  ylab("Percentage of Readings")

# Log Transform
ggplot(all, aes(x = log(reflected_par + 0.001))) +
  geom_histogram(aes(y = (..count..)/sum(..count..)), binwidth = 0.8) +
  scale_y_continuous(labels = scales::percent_format()) +
  labs(title = 'Reflected PAR - Log Scale') +
  xlab(expression('ln(micromol/m'^2*'/s)')) + 
  ylab("Percentage of Readings")

```

### Boxplot Analysis - Daytime Measurements Only

![Original Tolle Boxplots - 3c](./images/3c_plot_pic.JPG)

The boxplots from Figure 3[c] in the orginal paper are showing how the sensor measurements vary over the height of the tree. Tolle et al. are attempting to look for spatial gradients and trends by eliminating the temporal aspect of the data.

With regards to the PAR charts I don't believe these plots convey the right message. I think the author should have split up the temporal ranges included in this chart. For example, the PAR temporal range should only include daytime, when the sun is shining. Including the full temporal range adds a large number of data points with 0 light readings and effectively pulls the mass closer to zero for all readings and makes the gradient less pronounced.

We plotted the charts only considering daylight hours.

```{r, q5b-height-value-box-plots}
# try from 6:00 to 20:00
day_time = all %>%
  filter(time >= "06" & time <= "20")

# fix formatting for incident par boxplot and implement change for other dimensions (match paper)

# temperature

# humidity

# incident par
ggplot(day_time, aes(x=Height, y=incident_par, group = Height)) + 
    geom_boxplot() +
    coord_flip()

# reflected par

```


Some statement here about how the gradients over height become more clear when we strip out the night time measurements.


![Original Tolle Boxplots - 3d](./images/3d_plot_pic.JPG)


### Tolle Figure 4 Plot Suggestions
Any suggestions for improving the first two plots in Figure 4? Can you distinguish all the colors in these two plots?

I think discretizing the time plot (maybe showing every hour) would make the content more digestiable. Both charts are also lacking a legend and leave the onus on the reader to discern which heights correspond to which colors. To more clearly show height gradient we believe using a two-color continuum (shortest to tallest) would more clearly display the trend similar to what the author did in Figure 5.

### Tolle Figure 7 Comment
Comment on Figure 7. Is it possible to generate a better visualization to highlight the difference between network and log data?

We believe that overlaying the bar charts might create better contrast between the two sources. 


## Appendix

Extra Charts:

Duplicate record chart
```{r, duplicate-entries-by-source, include=FALSE}
# use primary composite key to plot duplicate readings by data source (same epoch and node id) there should only be one row per unique key in each data source
# plot 1 showing the breakdown by frequency (n is the number of duplicated rows)
all %>% 
  group_by(epoch, nodeid, data_source) %>%
  tally() %>%
  filter(n > 1) %>%
  ggplot(aes(n, fill = data_source)) +
    geom_histogram(binwidth = 1) +
    stat_bin(binwidth=1, geom="text", colour="black", size=3.5,
           aes(label=..count.., group=data_source), position=position_stack(vjust=0.5)) +
    ggtitle("Duplicate Entries by Data Source")
```


Histogram plots of all variables
```{r, all-hist-plots, include=FALSE}
# plot histograms of all numeric values
all %>%
  keep(is.numeric) %>% 
  gather() %>% 
  ggplot(aes(value)) + 
    facet_wrap(~ key, scales = "free") + 
    geom_histogram()
```


```{r}
#DEV CHUNKs


# validation that duplicate_entries has the right number of duplicates 
# all %>%
#   group_by(epoch, nodeid, data_source) %>%
#   tally() %>%
#   filter(n > 1) %>%
#   group_by(data_source) %>%
#   summarize(sum(n))


# # log with freq counts
# ggplot(all, aes(log(incident_par))) +
#   geom_histogram(fill = "blue", binwidth = .1, color = "white") +
#   ggtitle("Log Transform - Incident PAR")
# 
# # non-log plot with freq counts
# ggplot(all, aes(incident_par)) +
#   geom_histogram(fill = "blue", binwidth = 250, color = "white") +
#   ggtitle("No Log Transform - Incident PAR")
# 
# # log with percentages
# ggplot(all, aes(x = log(incident_par))) +
#   geom_histogram(aes(y = (..count..)/sum(..count..)), binwidth = .1) +
#   scale_y_continuous(labels = scales::percent_format()) +
#   ggtitle("Log Transform - Incident PAR (Percentage Scale)")
# 
# 
# # non-log with percentages
# ggplot(all, aes(x = incident_par)) +
#   geom_histogram(aes(y = (..count..)/sum(..count..)), binwidth = 250) +
#   scale_y_continuous(labels = scales::percent_format()) +
#   ggtitle("No Log Transform - Incident PAR (Percentage Scale)")
```

Dylan Date Code
```{r}
# all.4 = na.omit(all.3)
# missings = all.3[rowSums(is.na(all.3)) > 0, ]
# #summary(missings)
# missings.2 <- missings
# missings.2$result_time <-as.Date(missings.2$result_time)
# hist(missings.2$result_time, breaks = "months")
# 
# missings.3 = missings
# missings.3$result_time <-ymd_hms(missings.3$result_time)
# missings.3 %>% ggplot(aes(result_time, ..count..)) +
#     geom_histogram() +
#     theme_bw() + xlab(NULL) +
#     scale_x_datetime(breaks = date_breaks("3 months"), )
# 
# par(mfrow=c(3,3))
# # 25
# ggplot(missings.3, aes(x=result_time)) +
#   geom_histogram() +
#   scale_x_datetime(limits = c(ymd_hms("2004-05-25 00:00:01"),ymd_hms("2004-05-25 23:59:59")))
# 
# # 26
# ggplot(missings.3, aes(x=result_time)) +
#   geom_histogram() +
#   scale_x_datetime(limits = c(ymd_hms("2004-05-26 00:00:01"),ymd_hms("2004-05-26 23:59:59")))
# # 27
# ggplot(missings.3, aes(x=result_time)) +
#   geom_histogram() +
#   scale_x_datetime(limits = c(ymd_hms("2004-05-27 00:00:01"),ymd_hms("2004-05-27 23:59:59")))
# # 28
# ggplot(missings.3, aes(x=result_time)) +
#   geom_histogram() +
#   scale_x_datetime(limits = c(ymd_hms("2004-05-28 00:00:01"),ymd_hms("2004-05-28 23:59:59")))
# # 29
# ggplot(missings.3, aes(x=result_time)) +
#   geom_histogram() +
#   scale_x_datetime(limits = c(ymd_hms("2004-05-29 00:00:01"),ymd_hms("2004-05-29  23:59:59")))
# 
# #11-10
# ggplot(missings.3, aes(x=result_time)) +
#   geom_histogram() +
#   scale_x_datetime(limits = c(ymd_hms("2004-11-10 00:00:01"),ymd_hms("2004-11-10 23:59:59")))
```

